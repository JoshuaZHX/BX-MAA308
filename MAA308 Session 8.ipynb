{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VExhK4Iz32O"
   },
   "source": [
    "# Practical Session 8 - SegNet :  A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation - Tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CeXLI86z32Z"
   },
   "source": [
    "In this Practical Session, you will try to understand how to compute and run neural networks, and more precisely SegNet neural network, used to segment images with many different objects inside.\n",
    "You saw during lecture the original paper, of [Badrinarayanan et al.,2016](https://arxiv.org/pdf/1511.00561.pdf).    \n",
    "You will see here implementation and execution on a dataset named CamVid, as in the paper.\n",
    "\n",
    "\n",
    "Here you  have a link to the github of the original implementation of SegNet, with demos, articles references,etc : https://github.com/alexgkendall/SegNet-Tutorial .\n",
    "\n",
    "This notebook is mainly based on this [github](https://github.com/advaitsave/Multiclass-Semantic-Segmentation-CamVid/blob/master/Multiclass_Semantic_Segmentation_using_VGG_16_SegNet.ipynb).   \n",
    "As it is an encoder-decoder, and that several encoders already exist (VGG, ResNet), it is possible to create an encoder with VGG architecture for example, and then to load the weights of this well-known  encoder trained on same dataset. With this process you only have to learn the decoder (second part of SegNet network).  \n",
    "You can also do transfer learning which consists of : \n",
    "- using a full pre-trained network on a firts dataset for a first problem.\n",
    "- training it  on a new dataset for a related problem, and see if it adapts well.\n",
    "\n",
    "**Here we don't directly a pre-trained network (or encoder) to build SegNet. But we provide you saved weights of our training sessions of this network, with a cetain number of epochs. By yourself you can train the model with less or more epochs (it is very long), train after loading weights, try to modify the network and see effect on prediction, understand preprocessing steps, think about differences with the paper...**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQXpMQNL0Bke",
    "outputId": "ac14f3fc-d6ed-4a56-cd65-fc518a2aa5d3"
   },
   "outputs": [],
   "source": [
    "### TO USE IN GOOGLE COLAB\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6Qavnaqz32m"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.python.keras.callbacks import TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebpcOXQhz32s"
   },
   "source": [
    "# CamVid dataset importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Hyute2fz32t"
   },
   "outputs": [],
   "source": [
    "def _read_to_tensor(fname, output_height=224, output_width=224, normalize_data=False):\n",
    "    '''Function to read images from given image file path, and provide resized images as tensors\n",
    "        Inputs: \n",
    "            fname - image file path\n",
    "            output_height - required output image height\n",
    "            output_width - required output image width\n",
    "            normalize_data - if True, normalize data to be centered around 0 (mean 0, range 0 to 1)\n",
    "        Output: Processed image tensors\n",
    "    '''\n",
    "    \n",
    "    # Read the image as a tensor\n",
    "    img_strings = tf.io.read_file(fname)\n",
    "    imgs_decoded = tf.image.decode_jpeg(img_strings)\n",
    "    \n",
    "    # Resize the image\n",
    "    output = tf.image.resize(imgs_decoded, [output_height, output_width])\n",
    "    \n",
    "    # Normalize if required\n",
    "    if normalize_data:\n",
    "        output = (output - 128) / 128\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-2IyNMkTz32u"
   },
   "outputs": [],
   "source": [
    "img_dir = './data/CamSeq01/'\n",
    "# img_dir = '/content/drive/My Drive/X/MAA309/data/CamSeq01/' # for google colab\n",
    "\n",
    "# Required image dimensions\n",
    "output_height = 224\n",
    "output_width = 224\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nZqrit3z32v"
   },
   "source": [
    "## Reading frames and masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQZwkPAzz32w",
    "outputId": "7a35fd13-4569-4398-eb5a-b404c35fdd23"
   },
   "outputs": [],
   "source": [
    "def read_images(img_dir):\n",
    "    '''Function to get all image directories, read images and masks in separate tensors\n",
    "        Inputs: \n",
    "            img_dir - file directory\n",
    "        Outputs \n",
    "            frame_tensors, masks_tensors, frame files list, mask files list\n",
    "    '''\n",
    "    \n",
    "    # Get the file names list from provided directory\n",
    "    file_list = [f for f in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, f))]\n",
    "    \n",
    "    # Separate frame and mask files lists, exclude unnecessary files\n",
    "    frames_list = [file for file in file_list if ('_L' not in file) and ('txt' not in file) and ('.D' not in file)]\n",
    "    masks_list = [file for file in file_list if ('_L' in file) and ('txt' not in file) and ('.D' not in file)]\n",
    "    \n",
    "    frames_list.sort()\n",
    "    masks_list.sort()\n",
    "    \n",
    "    print('{} frame files found in the provided directory.'.format(len(frames_list)))\n",
    "    print('{} mask files found in the provided directory.'.format(len(masks_list)))\n",
    "    \n",
    "    # Create file paths from file names\n",
    "    frames_paths = [os.path.join(img_dir, fname) for fname in frames_list]\n",
    "    masks_paths = [os.path.join(img_dir, fname) for fname in masks_list]\n",
    "    \n",
    "    # Create dataset of tensors\n",
    "    frame_data = tf.data.Dataset.from_tensor_slices(frames_paths)\n",
    "    masks_data = tf.data.Dataset.from_tensor_slices(masks_paths)\n",
    "    \n",
    "    # Read images into the tensor dataset\n",
    "    frame_tensors = frame_data.map(_read_to_tensor)\n",
    "    masks_tensors = masks_data.map(_read_to_tensor)\n",
    "    \n",
    "    print('Completed importing {} frame images from the provided directory.'.format(len(frames_list)))\n",
    "    print('Completed importing {} mask images from the provided directory.'.format(len(masks_list)))\n",
    "    \n",
    "    return frame_tensors, masks_tensors, frames_list, masks_list\n",
    "\n",
    "frame_tensors, masks_tensors, frames_list, masks_list = read_images(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96ad_lfxz32x"
   },
   "outputs": [],
   "source": [
    "# Make an iterator to extract images from the tensor dataset\n",
    "\n",
    "frame_batches = tf.compat.v1.data.make_one_shot_iterator(frame_tensors)  \n",
    "mask_batches = tf.compat.v1.data.make_one_shot_iterator(masks_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0SkzfknDz32y",
    "outputId": "8ee0e17a-101a-4089-c6b2-778ff45682cc"
   },
   "outputs": [],
   "source": [
    "n_images_to_show = 5\n",
    "\n",
    "for i in range(n_images_to_show):\n",
    "    \n",
    "    # Get the next image from iterator\n",
    "    frame = frame_batches.get_next().numpy().astype(np.uint8)\n",
    "    mask = mask_batches.get_next().numpy().astype(np.uint8)\n",
    "    \n",
    "    #Plot the corresponding frames and masks\n",
    "    fig = plt.figure(figsize=(20,7))\n",
    "    fig.add_subplot(1,2,1)\n",
    "    plt.imshow(frame)\n",
    "    fig.add_subplot(1,2,2)\n",
    "    plt.imshow(mask)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQPuHlc-z320",
    "outputId": "32fd7a93-ed37-4115-b687-138107576451"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = './data/CamSeq01/'\n",
    "# DATA_PATH = '/content/drive/My Drive/X/MAA309/data/CamSeq01/'\n",
    "\n",
    "# Create folders to hold images and masks\n",
    "\n",
    "folders = ['train_frames/train', 'train_masks/train', 'val_frames/val', 'val_masks/val','frames/','masks/']\n",
    "\n",
    "\n",
    "for folder in folders:\n",
    "    try:\n",
    "        os.makedirs(DATA_PATH + folder)\n",
    "    except Exception as e: print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wx4Y-kp9z321",
    "outputId": "bb2a75c1-9e79-4d1c-9449-161faf52638f"
   },
   "outputs": [],
   "source": [
    "def generate_image_folder_structure(frames, masks, frames_list, masks_list):\n",
    "    '''Function to save images in the appropriate folder directories \n",
    "        Inputs :\n",
    "        -----------\n",
    "        frames - frame tensor dataset\n",
    "        masks - mask tensor dataset\n",
    "        frames_list - frame file paths\n",
    "        masks_list - mask file paths\n",
    "    '''\n",
    "    #Create iterators for frames and masks\n",
    "    frame_batches = tf.compat.v1.data.make_one_shot_iterator(frames)\n",
    "    mask_batches = tf.compat.v1.data.make_one_shot_iterator(masks)\n",
    "    \n",
    "    #Iterate over the train images while saving the frames and masks in appropriate folders\n",
    "    dir_name='train'\n",
    "  \n",
    "    for file in zip(frames_list[:-round(0.2*len(frames_list))],masks_list[:-round(0.2*len(masks_list))]):\n",
    "        \n",
    "            \n",
    "        #Convert tensors to numpy arrays\n",
    "        frame = frame_batches.get_next().numpy().astype(np.uint8)\n",
    "        mask = mask_batches.get_next().numpy().astype(np.uint8)\n",
    "        \n",
    "        #Convert numpy arrays to images\n",
    "        frame = Image.fromarray(frame)\n",
    "        mask = Image.fromarray(mask)\n",
    "        \n",
    "        #Save frames and masks to correct directories\n",
    "        frame.save(DATA_PATH+'{}_frames/{}'.format(dir_name,dir_name)+'/'+file[0])\n",
    "        mask.save(DATA_PATH+'{}_masks/{}'.format(dir_name,dir_name)+'/'+file[1])\n",
    "        \n",
    "        frame.save(DATA_PATH+'frames/'+'/'+file[0])\n",
    "        mask.save(DATA_PATH+'masks/'+'/'+file[1])\n",
    "    \n",
    "    #Iterate over the val images while saving the frames and masks in appropriate folders\n",
    "    dir_name='val'\n",
    "    \n",
    "    for file in zip(frames_list[-round(0.2*len(frames_list)):],masks_list[-round(0.2*len(masks_list)):]):\n",
    "        \n",
    "        \n",
    "        #Convert tensors to numpy arrays\n",
    "        frame = frame_batches.next().numpy().astype(np.uint8)\n",
    "        mask = mask_batches.next().numpy().astype(np.uint8)\n",
    "        \n",
    "        #Convert numpy arrays to images\n",
    "        frame = Image.fromarray(frame)\n",
    "        mask = Image.fromarray(mask)\n",
    "        \n",
    "        #Save frames and masks to correct directories\n",
    "        frame.save(DATA_PATH+'{}_frames/{}'.format(dir_name,dir_name)+'/'+file[0])\n",
    "        mask.save(DATA_PATH+'{}_masks/{}'.format(dir_name,dir_name)+'/'+file[1])\n",
    "        \n",
    "        frame.save(DATA_PATH+'frames/'+'/'+file[0])\n",
    "        mask.save(DATA_PATH+'masks/'+'/'+file[1])\n",
    "    \n",
    "    print(\"Saved {} frames to directory {}\".format(len(frames_list),DATA_PATH))\n",
    "    print(\"Saved {} masks to directory {}\".format(len(masks_list),DATA_PATH))\n",
    "    \n",
    "generate_image_folder_structure(frame_tensors, masks_tensors, frames_list, masks_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNxm_qkVz323"
   },
   "source": [
    "\n",
    "# Create useful label and code conversion dictionaries\n",
    "\n",
    "These will be used for:\n",
    "\n",
    "- One hot encoding the mask labels for model training\n",
    "- Decoding the predicted labels for interpretation and visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neWy5OR4z324"
   },
   "source": [
    "## Function to parse the file \"label_colors.txt\" which contains the class definitions\n",
    "You obtain a list with colors, and a list with corresponding object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LKYShO2z324"
   },
   "outputs": [],
   "source": [
    "def parse_code(l):\n",
    "    '''Function to parse lines in a text file, returns separated elements (label codes and names in this case)\n",
    "    '''\n",
    "    if len(l.strip().split(\"\\t\")) == 2:\n",
    "        a, b = l.strip().split(\"\\t\")\n",
    "        return tuple(int(i) for i in a.split(' ')), b\n",
    "    else:\n",
    "        a, b, c = l.strip().split(\"\\t\")\n",
    "        return tuple(int(i) for i in a.split(' ')), c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecWWovKAz325",
    "outputId": "7e7fc556-6885-415f-f2a5-8a1a846d6fc4"
   },
   "outputs": [],
   "source": [
    "label_codes, label_names = zip(*[parse_code(l) for l in open(img_dir+\"label_colors.txt\")])\n",
    "label_codes, label_names = list(label_codes), list(label_names)\n",
    "label_codes[:5], label_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GejSpRmez326"
   },
   "source": [
    "##  Define functions for one hot encoding rgb labels, and decoding encoded predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tUhrV0Gz326"
   },
   "outputs": [],
   "source": [
    "code2id = {v:k for k,v in enumerate(label_codes)}\n",
    "id2code = {k:v for k,v in enumerate(label_codes)}\n",
    "name2id = {v:k for k,v in enumerate(label_names)}\n",
    "id2name = {k:v for k,v in enumerate(label_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykN-Ef2Tz327"
   },
   "outputs": [],
   "source": [
    "def rgb_to_onehot(rgb_image, colormap = id2code):\n",
    "    '''Function to one hot encode RGB mask labels\n",
    "        Inputs: \n",
    "            rgb_image - image matrix (eg. 256 x 256 x 3 dimension numpy ndarray)\n",
    "            colormap - dictionary of color to label id\n",
    "        Output: One hot encoded image of dimensions (height x width x num_classes) where num_classes = len(colormap)\n",
    "    '''\n",
    "    num_classes = len(colormap)\n",
    "    shape = rgb_image.shape[:2]+(num_classes,)\n",
    "    encoded_image = np.zeros( shape, dtype=np.int8 )\n",
    "    for i, cls in enumerate(colormap):\n",
    "        encoded_image[:,:,i] = np.all(rgb_image.reshape( (-1,3) ) == colormap[i], axis=1).reshape(shape[:2])\n",
    "    return encoded_image\n",
    "\n",
    "\n",
    "def onehot_to_rgb(onehot, colormap = id2code):\n",
    "    '''Function to decode encoded mask labels\n",
    "        Inputs: \n",
    "            onehot - one hot encoded image matrix (height x width x num_classes)\n",
    "            colormap - dictionary of color to label id\n",
    "        ------------\n",
    "\n",
    "        Output: Decoded RGB image (height x width x 3) \n",
    "    '''\n",
    "    single_layer = np.argmax(onehot, axis=-1)\n",
    "    output = np.zeros( onehot.shape[:2]+(3,) )\n",
    "    for k in colormap.keys():\n",
    "        output[single_layer==k] = colormap[k]\n",
    "    return np.uint8(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIYAAwITz327"
   },
   "source": [
    "#  Creating custom Image data generators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7PZprf9z327"
   },
   "outputs": [],
   "source": [
    "# Normalizing only frame images, since masks contain label info\n",
    "data_gen_args = dict(rescale=1./255.)\n",
    "mask_gen_args = dict()\n",
    "\n",
    "train_frames_datagen = ImageDataGenerator(**data_gen_args)\n",
    "train_masks_datagen = ImageDataGenerator(**mask_gen_args)\n",
    "val_frames_datagen = ImageDataGenerator(**data_gen_args)\n",
    "val_masks_datagen = ImageDataGenerator(**mask_gen_args)\n",
    "full_frames_datagen = ImageDataGenerator(**data_gen_args)\n",
    "full_masks_datagen = ImageDataGenerator(**mask_gen_args)\n",
    "\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty4e_nzRz328"
   },
   "source": [
    "\n",
    "## Custom image data generators for creating batches of frames and masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EPUjv53z328"
   },
   "outputs": [],
   "source": [
    "def TrainAugmentGenerator(seed = 1, batch_size = 5):\n",
    "    '''Train Image data generator\n",
    "        ------------\n",
    "        Inputs: \n",
    "        seed - seed provided to the flow_from_directory function to ensure aligned data flow\n",
    "        batch_size - number of images to import at a time\n",
    "        ------------\n",
    "        Output: Decoded RGB image (height x width x 3) \n",
    "    '''\n",
    "    train_image_generator = train_frames_datagen.flow_from_directory(DATA_PATH + 'train_frames/', batch_size = batch_size, seed = seed, target_size = (224, 224))\n",
    "    train_mask_generator = train_masks_datagen.flow_from_directory(DATA_PATH + 'train_masks/', batch_size = batch_size, seed = seed, target_size = (224, 224))\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        X1i = train_image_generator.next()\n",
    "        X2i = train_mask_generator.next()\n",
    "\n",
    "        #One hot encoding RGB images\n",
    "        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n",
    "        yield X1i[0], np.asarray(mask_encoded)\n",
    "\n",
    "def ValAugmentGenerator(seed = 1, batch_size = 5):\n",
    "    '''Validation Image data generator\n",
    "    ------------\n",
    "    Inputs: \n",
    "        seed - seed provided to the flow_from_directory function to ensure aligned data flow\n",
    "        batch_size - number of images to import at a time\n",
    "    ------------\n",
    "    Output: Decoded RGB image (height x width x 3) \n",
    "    \n",
    "    '''\n",
    "    val_image_generator = val_frames_datagen.flow_from_directory(DATA_PATH + 'val_frames/', batch_size = batch_size, seed = seed, target_size = (224, 224))\n",
    "    val_mask_generator = val_masks_datagen.flow_from_directory(DATA_PATH + 'val_masks/',batch_size = batch_size, seed = seed, target_size = (224, 224))\n",
    "\n",
    "\n",
    "    while True:\n",
    "        X1i = val_image_generator.next()\n",
    "        X2i = val_mask_generator.next()\n",
    "        \n",
    "        #One hot encoding RGB images\n",
    "        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n",
    "        \n",
    "        yield X1i[0], np.asarray(mask_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQTknxljz32-"
   },
   "source": [
    "# Create network SegNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLvVMvPHz32-"
   },
   "outputs": [],
   "source": [
    "def Segnet(n_classes,input_height=224, input_width=224 , kernel=3):\n",
    "\n",
    "    img_input = Input(shape=(input_height,input_width,3))\n",
    "\n",
    "    x = Conv2D(64, (kernel, kernel), padding='same', name='block1_conv1', data_format='channels_last' )(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu') (x)\n",
    "    x = Conv2D(64, (kernel, kernel), padding='same', name='block1_conv2', data_format='channels_last' )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu') (x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool', data_format='channels_last' )(x)\n",
    "    f1 = x\n",
    "    \n",
    "    \n",
    "    # Block 2\n",
    "    x = Conv2D(128, (kernel, kernel), padding='same', name='block2_conv1', data_format='channels_last' )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu') (x)\n",
    "    x = Conv2D(128, (kernel, kernel), padding='same', name='block2_conv2', data_format='channels_last' )(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool', data_format='channels_last' )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu') (x)\n",
    "    f2 = x\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (kernel, kernel), padding='same', name='block3_conv1', data_format='channels_last' )(x)\n",
    "    x = ( BatchNormalization())(x)\n",
    "    x = Activation('relu') (x)\n",
    "    x = Conv2D(256, (kernel, kernel), padding='same', name='block3_conv2', data_format='channels_last' )(x)\n",
    "    x = ( BatchNormalization())(x)\n",
    "    x = Activation('relu') (x)\n",
    "    x = Conv2D(256, (kernel, kernel), padding='same', name='block3_conv3', data_format='channels_last' )(x)\n",
    "    x = ( BatchNormalization())(x)\n",
    "    x = Activation('relu') (x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool', data_format='channels_last' )(x)\n",
    "    f3 = x\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (kernel, kernel), padding='same', name='block4_conv1', data_format='channels_last' )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu') (x)\n",
    "    x = Conv2D(512, (kernel, kernel), padding='same', name='block4_conv2', data_format='channels_last' )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu') (x)\n",
    "    x = Conv2D(512, (kernel, kernel), padding='same', name='block4_conv3', data_format='channels_last' )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu') (x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool', data_format='channels_last' )(x)\n",
    "    f4 = x\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (kernel, kernel), padding='same', name='block5_conv1', data_format='channels_last' )(x)\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Activation('relu') (x)\n",
    "    x = Conv2D(512, (kernel, kernel), padding='same', name='block5_conv2', data_format='channels_last' )(x)\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Activation('relu') (x)\n",
    "    x = Conv2D(512, (kernel, kernel), padding='same', name='block5_conv3', data_format='channels_last' )(x)\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Activation('relu') (x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool', data_format='channels_last' )(x)\n",
    "    f5 = x\n",
    "\n",
    "\n",
    "    o = f4 # or f5, you can change it.\n",
    "\n",
    "    o = ( UpSampling2D( (2,2), data_format='channels_last'))(o)\n",
    "    o = ( Conv2D( 512, (kernel, kernel), padding='same', data_format='channels_last'))(o)\n",
    "    o = ( BatchNormalization())(o)\n",
    "   \n",
    "    o = ( UpSampling2D( (2,2), data_format='channels_last'))(o)\n",
    "    o = ( Conv2D( 256, (kernel, kernel), padding='same', data_format='channels_last')) (o)\n",
    "    o = ( BatchNormalization())(o)\n",
    "   \n",
    "    o = ( UpSampling2D((2,2)  , data_format='channels_last' ) )(o)\n",
    "    o = ( Conv2D( 128 , (kernel, kernel), padding='same' , data_format='channels_last' )) (o)\n",
    "    o = ( BatchNormalization())(o)\n",
    " \n",
    "    o = ( UpSampling2D((2,2)  , data_format='channels_last' ))(o)\n",
    "    o = ( Conv2D( 64 , (kernel, kernel), padding='same'  , data_format='channels_last' )) (o)\n",
    "    o = ( BatchNormalization())(o)\n",
    "\n",
    "    o =  Conv2D( n_classes , (kernel, kernel) , padding='same', data_format='channels_last' )(o)\n",
    "    o = (Activation('softmax')) (o)\n",
    "    \n",
    "    model = Model(img_input,o)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q09j2cD2z32_"
   },
   "outputs": [],
   "source": [
    "## There are 32 possible classes.\n",
    "model = Segnet(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdMeH-Zsz32_",
    "outputId": "a4f51ef6-6e23-4fa1-e9e0-8ed503b6cf6b"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ut69rF62z32_"
   },
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiQaPwF-z33A"
   },
   "outputs": [],
   "source": [
    "# Categorical crossentropy loss since labels have been one hot encoded\n",
    "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kawDOpBcz33A"
   },
   "outputs": [],
   "source": [
    "## Allows to stop training before the end if some condition is reached. Here we stop if after 10 epochs the loss of validation set is not decreasing anymore.\n",
    "es = EarlyStopping(mode='min', monitor='val_loss', patience=30, verbose=1)\n",
    "callbacks = [es]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrTEC572z33A"
   },
   "source": [
    "# Train model or load weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leLuqV5Sz33B"
   },
   "source": [
    "## Load weights of pre-trained model\n",
    "**Here** you can load pre-trained weights of the model, after coding and compiling it. Then, you can train it, it will consider existing weights as baseline, initialisation, and you can obtain results faster than if you train from beginning. \n",
    "So skip \"Train model\" part if you just want to see results.  \n",
    "Or change `num_epochs` if you want to see evolution of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q50eaVR5z33C",
    "outputId": "74106fab-8cad-4b95-e548-4811a90fd93b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.load_weights(img_dir+\"../model_camvid_weight_ep85.hdf5\")\n",
    "# model.evaluate(x=TrainAugmentGenerator(),steps=1)\n",
    "# model.evaluate(x=ValAugmentGenerator(),steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjHStWOEz33D"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvViFnkUz33D"
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "steps_per_epoch = np.ceil(float(len(frames_list) - round(0.2*len(frames_list))) / float(batch_size))\n",
    "validation_steps = np.ceil(0.2*len(frames_list) / float(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvhYL1qCz33E",
    "outputId": "6846afa4-e4f8-45a6-ee52-3700ef388598",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "num_epochs = 200 ## depending on if you load weights or not\n",
    "result = model.fit(x=TrainAugmentGenerator(batch_size=batch_size), batch_size=batch_size, steps_per_epoch=int(steps_per_epoch), validation_data = ValAugmentGenerator(batch_size=batch_size),validation_steps=int(validation_steps),epochs=num_epochs, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-FR6fpNz33E"
   },
   "outputs": [],
   "source": [
    "model.save_weights(DATA_PATH+'../model_camvid_weight_ep{0}.hdf5'.format(your_nb_epochs)) \n",
    "\n",
    "# if you load weights of pre-trained model with 100 epochs, and that you train again after,\n",
    "# you will have (100 + num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSZ8QCGJz33F"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2C1SCzez33F"
   },
   "source": [
    "## Accuracy and Loss plots\n",
    "You CAN NOT see evolutions of loss and accuracy if you don't train your model. \n",
    "If you want to see directly prediction from pre-trained network with loaded weights, skip this part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "J0NAgnnYz33G",
    "outputId": "66f19082-a672-4388-dc15-db56ad76af74"
   },
   "outputs": [],
   "source": [
    "# Get actual number of epochs model was trained for\n",
    "N = len(result.history['loss'])\n",
    "\n",
    "#Plot the model evaluation history\n",
    "plt.style.use(\"ggplot\")\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.title(\"Losses\")\n",
    "plt.plot(np.arange(0, N), result.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), result.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.title(\"Accuracies\")\n",
    "plt.plot(np.arange(0, N), result.history[\"accuracy\"], label=\"train_accuracy\")\n",
    "plt.plot(np.arange(0, N), result.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxzoEMWnz33H"
   },
   "source": [
    "\n",
    "## Extract and display model frame, prediction and mask batch¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmSlBWQNz33H"
   },
   "source": [
    "Here you can see predictions of SegNet network on validation set : \n",
    "- just with loaded weights for 100 epochs, without training again, see obtained segmentation, not so good but a start.\n",
    "- after more training (so more than 100 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CK-v88Irz33I"
   },
   "outputs": [],
   "source": [
    "training_gen = TrainAugmentGenerator()\n",
    "testing_gen = ValAugmentGenerator(batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31tT8404E_v0",
    "outputId": "705eff5a-9cf6-4e56-9821-be50c6dd47d0"
   },
   "outputs": [],
   "source": [
    "batch_img,batch_mask = next(testing_gen)\n",
    "pred_all= model.predict(batch_img)\n",
    "np.shape(pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mjxxxyB9z33J",
    "outputId": "2d90fa9e-36b3-40e4-a411-daaa7e55f7f1"
   },
   "outputs": [],
   "source": [
    " for i in range(0,np.shape(pred_all)[0]):\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "    \n",
    "    ax1 = fig.add_subplot(1,3,1)\n",
    "    ax1.imshow(batch_img[i])\n",
    "    ax1.title.set_text('Actual frame')\n",
    "    ax1.grid(b=None)\n",
    "    \n",
    "    \n",
    "    ax2 = fig.add_subplot(1,3,2)\n",
    "    ax2.set_title('Ground truth labels')\n",
    "    ax2.imshow(onehot_to_rgb(batch_mask[i],id2code))\n",
    "    ax2.grid(b=None)\n",
    "    \n",
    "    ax3 = fig.add_subplot(1,3,3)\n",
    "    ax3.set_title('Predicted labels')\n",
    "    ax3.imshow(onehot_to_rgb(pred_all[i],id2code))\n",
    "    ax3.grid(b=None)\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Practical Session 8 - SegNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
